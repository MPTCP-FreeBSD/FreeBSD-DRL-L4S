{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from gym import wrappers\n",
    "from gym import spaces\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_custom=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sklearn=pd.read_csv(\"data1.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burst_allowance</th>\n",
       "      <th>drop_prob</th>\n",
       "      <th>current_qdelay</th>\n",
       "      <th>qdelay_old</th>\n",
       "      <th>accu_prob</th>\n",
       "      <th>measurement_start</th>\n",
       "      <th>tot_pkts</th>\n",
       "      <th>tot_bytes</th>\n",
       "      <th>length</th>\n",
       "      <th>len_bytes</th>\n",
       "      <th>drops</th>\n",
       "      <th>ecn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.019544</td>\n",
       "      <td>0.018578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.195665</td>\n",
       "      <td>0.247684</td>\n",
       "      <td>0.022801</td>\n",
       "      <td>0.025829</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-1.2500</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.999920</td>\n",
       "      <td>-1.2500</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.035831</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.999985</td>\n",
       "      <td>-0.9375</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.195739</td>\n",
       "      <td>0.247724</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.137096</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4755 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      burst_allowance  drop_prob  current_qdelay  qdelay_old  accu_prob  \\\n",
       "0            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "1            1.000000  -1.000000         -0.0000    0.000000        0.0   \n",
       "2            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "3            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "4            0.888889  -0.999990         -0.3125    0.020833        0.0   \n",
       "...               ...        ...             ...         ...        ...   \n",
       "4750         0.777778  -1.000000         -0.6250    0.041667        0.0   \n",
       "4751         0.555556  -0.999971         -1.2500    0.083333        0.0   \n",
       "4752         0.444444  -0.999920         -1.2500    0.083333        0.0   \n",
       "4753         0.666667  -0.999985         -0.9375    0.062500        0.0   \n",
       "4754         0.333333  -1.000000         -0.0000    0.000000        0.0   \n",
       "\n",
       "      measurement_start  tot_pkts  tot_bytes    length  len_bytes     drops  \\\n",
       "0              0.001496  0.000392   0.000287  0.003257   0.003690  0.000000   \n",
       "1              0.001621  0.000637   0.000568  0.000000   0.000000  0.000000   \n",
       "2              0.002494  0.001201   0.001013  0.013029   0.011198  0.000000   \n",
       "3              0.002660  0.001618   0.001512  0.019544   0.018578  0.000000   \n",
       "4              0.002660  0.001716   0.001636  0.003257   0.003690  0.000000   \n",
       "...                 ...       ...        ...       ...        ...       ...   \n",
       "4750           0.998670  0.195665   0.247684  0.022801   0.025829  0.435841   \n",
       "4751           0.998545  0.136900   0.027735  0.087948   0.042636  0.000000   \n",
       "4752           0.998545  0.136900   0.027735  0.035831   0.022779  0.000000   \n",
       "4753           0.998670  0.195739   0.247724  0.013029   0.008369  0.435841   \n",
       "4754           0.998545  0.137096   0.027774  0.000000   0.000000  0.000000   \n",
       "\n",
       "      ecn  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "...   ...  \n",
       "4750    1  \n",
       "4751    0  \n",
       "4752    0  \n",
       "4753    1  \n",
       "4754    0  \n",
       "\n",
       "[4755 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Custom Gym Environment\n",
    "observations = ['burst_allowance',\n",
    " 'drop_prob',\n",
    " 'current_qdelay',\n",
    " 'qdelay_old',\n",
    " 'accu_prob',\n",
    " 'measurement_start',\n",
    " 'tot_pkts',\n",
    " 'tot_bytes',\n",
    " 'length',\n",
    " 'len_bytes',\n",
    " 'drops',\n",
    " 'ecn']\n",
    "\n",
    "observations=list(df_sklearn.columns)\n",
    "\n",
    "actions=['new_drop_prob']\n",
    "\n",
    "states_record=[]\n",
    "\n",
    "states_dict={}\n",
    "\n",
    "global df_sklearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tcp_obs_space():\n",
    "    lower_obs_bound = {\n",
    "        'burst_allowance': - np.inf,\n",
    "        'drop_prob': - np.inf,\n",
    "        'current_qdelay': - np.inf,\n",
    "        'qdelay_old': - np.inf,\n",
    "        'accu_prob': - np.inf,\n",
    "        'measurement_start': - np.inf,\n",
    "        'tot_pkts': - np.inf,\n",
    "        'tot_bytes': - np.inf,\n",
    "        'length': - np.inf,\n",
    "        'len_bytes': - np.inf,\n",
    "        'drops': - np.inf,\n",
    "        'ecn': - np.inf\n",
    "        \n",
    "    }\n",
    "\n",
    "    higher_obs_bound = {\n",
    "        'burst_allowance': np.inf,\n",
    "        'drop_prob': np.inf,\n",
    "        'current_qdelay': np.inf,\n",
    "        'qdelay_old': np.inf,\n",
    "        'accu_prob': np.inf,\n",
    "        'measurement_start': np.inf,\n",
    "        'tot_pkts': np.inf,\n",
    "        'tot_bytes': np.inf,\n",
    "        'length': np.inf,\n",
    "        'len_bytes': np.inf,\n",
    "        'drops': np.inf,\n",
    "        'ecn': np.inf\n",
    "        \n",
    "    }\n",
    "\n",
    "    low = np.array([lower_obs_bound[o] for o in observations])\n",
    "    high = np.array([higher_obs_bound[o] for o in observations])\n",
    "    shape = (len(observations),)\n",
    "    return gym.spaces.Box(low,high,shape)\n",
    "\n",
    "\n",
    "def tcp_action_space():\n",
    "    lower_obs_bound = {\n",
    "        'new_drop_prob': - np.inf,\n",
    "    }\n",
    "    higher_obs_bound = {\n",
    "        'new_drop_prob': np.inf,\n",
    "    }\n",
    "\n",
    "    low = np.array([lower_obs_bound[o] for o in actions])\n",
    "    high = np.array([higher_obs_bound[o] for o in actions])\n",
    "    shape = (len(observations),)\n",
    "    return gym.spaces.Box(low,high,shape)\n",
    "\n",
    "class gym_env():\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self,filename):\n",
    "\n",
    "        self.filename=filename\n",
    "        \n",
    "        df_sklearn=pd.read_csv(filename,index_col=0)\n",
    "        display(df_sklearn)\n",
    "\n",
    "        \n",
    "      \n",
    "      \n",
    "\n",
    "        \n",
    "\n",
    "        # self.rate=df[\"rate\"][0]\n",
    "        # self.total_rate=df[\"total_rate\"][0]\n",
    "        # self.sent=df[\"sent\"][0]\n",
    "        # self.lost= df[\"lost\"][0]\n",
    "        # self.util= df[\"lost\"][0]\n",
    "        # self.old_util= df[\"old_util\"][0]\n",
    "        # self.thpt= df[\"thpt\"][0]\n",
    "        # self.loss_rate= df[\"loss_rate\"][0]\n",
    "        self.observations = ['burst_allowance','drop_prob','current_qdelay','qdelay_old','accu_prob',\n",
    "                             'measurement_start','tot_pkts','tot_bytes','length','len_bytes','drops','ecn']\n",
    "\n",
    "        \n",
    "        self.observation_space = tcp_obs_space()\n",
    "\n",
    "        self.count=0\n",
    "\n",
    "        self.state_nos=0\n",
    "        self.temp_states_list=[]\n",
    "        \n",
    "        self.action_space = spaces.Box(low=-np.inf,high=np.inf,dtype=np.float32,shape=(1,))\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def observation(self):\n",
    "        return np.array([self.state[o] for o in self.observations])\n",
    "\n",
    "    def reward(self):\n",
    "        return self.state['current_qdelay']\n",
    "\n",
    "    \"\"\" reset env, return the initial state  \"\"\"\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        # self.state={\n",
    "        # 'rate': 0,\n",
    "        # 'total_rate': 0,\n",
    "        # 'sent': 0,\n",
    "        # 'lost': 0,\n",
    "        # 'util': 0,\n",
    "        # 'old_util': 0,\n",
    "        # 'thpt': 0,\n",
    "        # 'loss_rate': 0,\n",
    "        # }\n",
    "        # self.state={\n",
    "        # 'rate': df[\"rate\"][random.randint(0, df.shape[0])],\n",
    "        # 'total_rate': df[\"total_rate\"][random.randint(0, df.shape[0])],\n",
    "        # 'sent': df[\"sent\"][random.randint(0, df.shape[0])],\n",
    "        # 'lost': df[\"lost\"][random.randint(0, df.shape[0])],\n",
    "        # 'util': df[\"util\"][random.randint(0, df.shape[0])],\n",
    "        # 'old_util': df[\"old_util\"][random.randint(0, df.shape[0])],\n",
    "        # 'thpt': df[\"thpt\"][random.randint(0, df.shape[0])],\n",
    "        # 'loss_rate': df[\"loss_rate\"][random.randint(0, df.shape[0])]\n",
    "        # }\n",
    "\n",
    "        rindex=random.randint(0, df_sklearn.shape[0]-1)\n",
    "\n",
    "        \n",
    "        self.state={\n",
    "            'burst_allowance': df_sklearn.iloc[rindex][\"burst_allowance\"],\n",
    "            'drop_prob': df_sklearn.iloc[rindex][\"drop_prob\"],\n",
    "            'current_qdelay': df_sklearn.iloc[rindex][\"current_qdelay\"],\n",
    "            'qdelay_old': df_sklearn.iloc[rindex][\"qdelay_old\"],\n",
    "            'accu_prob': df_sklearn.iloc[rindex][\"accu_prob\"],\n",
    "            'measurement_start': df_sklearn.iloc[rindex][\"measurement_start\"],\n",
    "            'tot_pkts': df_sklearn.iloc[rindex][\"tot_pkts\"],\n",
    "            'tot_bytes': df_sklearn.iloc[rindex][\"tot_bytes\"],\n",
    "            'length': df_sklearn.iloc[rindex][\"length\"],\n",
    "            'len_bytes': df_sklearn.iloc[rindex][\"len_bytes\"],\n",
    "            'drops': df_sklearn.iloc[rindex][\"drops\"],\n",
    "            'ecn': df_sklearn.iloc[rindex][\"ecn\"]\n",
    "        }\n",
    "        print(\"reset happened\")\n",
    "        print(df_sklearn.iloc[rindex]['current_qdelay'])\n",
    "        self.state_nos=0\n",
    "        self.temp_states_list=[]\n",
    "        self.count=self.count+1\n",
    "\n",
    "        states_record.append(self.state)\n",
    "        self.temp_states_list.append(self.state)\n",
    "        return self.observation()\n",
    "        \n",
    "\n",
    "    \"\"\" action = [sub1_buff_size, sub2_buff_size] \"\"\"\n",
    "    def step(self, action):\n",
    "        # A = [self.fd, action[0], action[1]]\n",
    "        # mpsched.set_seg(A)\n",
    "        time.sleep(2)\n",
    "        done = False\n",
    "        \n",
    "        \n",
    "        #state_nxt = mpsched.get_sub_info(self.fd)\n",
    "\n",
    "        input=action[0]\n",
    "\n",
    "        if(math.isnan(input) or input<-1 or input >1):\n",
    "          input=0\n",
    "          print(\"step happened\")\n",
    "          print(\"Wrong action value\")\n",
    "          print(input)\n",
    "          return self.observation(), self.reward(), done, {}\n",
    "\n",
    "\n",
    "        df_sort = df_sklearn.iloc[(df_sklearn['drop_prob']-input).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "\n",
    "        state_nxt={\n",
    "            'burst_allowance': df_sort[\"burst_allowance\"].values[0],\n",
    "            'drop_prob': df_sort[\"drop_prob\"].values[0],\n",
    "            'current_qdelay': df_sort[\"current_qdelay\"].values[0],\n",
    "            'qdelay_old': df_sort[\"qdelay_old\"].values[0],\n",
    "            'accu_prob': df_sort[\"accu_prob\"].values[0],\n",
    "            'measurement_start': df_sort[\"measurement_start\"].values[0],\n",
    "            'tot_pkts': df_sort[\"tot_pkts\"].values[0],\n",
    "            'tot_bytes': df_sort[\"tot_bytes\"].values[0],\n",
    "            'length': df_sort[\"length\"].values[0],\n",
    "            'len_bytes': df_sort[\"len_bytes\"].values[0],\n",
    "            'drops': df_sort[\"drops\"].values[0],\n",
    "            'ecn': df_sort[\"ecn\"].values[0]\n",
    "        }\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        if len(state_nxt) == 0:\n",
    "            done = True\n",
    "\n",
    "\n",
    "        self.state_nos=self.state_nos+1\n",
    "\n",
    "        if(self.state_nos>100):\n",
    "            done=True\n",
    "\n",
    "       \n",
    "        if self.state['current_qdelay']==(df_sklearn[\"current_qdelay\"].max()+1):\n",
    "            done = True\n",
    "\n",
    "        # if(df_sort[\"total_rate\"].values[0]==470000):\n",
    "        #     rindex=random.randint(0, df_temp.shape[0]-1)\n",
    "\n",
    "        #     df_temp.iloc[rindex]\n",
    "        #     self.state={\n",
    "        #     'rate': df_temp.iloc[rindex][\"rate\"],\n",
    "        #     'total_rate': df_temp.iloc[rindex][\"total_rate\"],\n",
    "        #     'sent': df_temp.iloc[rindex][\"sent\"],\n",
    "        #     'lost': df_temp.iloc[rindex][\"lost\"],\n",
    "        #     'util': df_temp.iloc[rindex][\"util\"],\n",
    "        #     'old_util': df_temp.iloc[rindex][\"old_util\"],\n",
    "        #     'thpt': df_temp.iloc[rindex][\"thpt\"],\n",
    "        #     'loss_rate': df_temp.iloc[rindex][\"loss_rate\"]\n",
    "        #     }\n",
    "\n",
    "        if done==True:              \n",
    "            states_dict[str(self.count)]=self.temp_states_list\n",
    "            self.temp_states_list=[]\n",
    "\n",
    "        # print(\"step happened\")\n",
    "        # print(\"action valu recieved\")\n",
    "        # print(input)\n",
    "        # print(state_nxt)\n",
    "        print(\"self.state_nos\",self.state_nos)\n",
    "        # print()\n",
    "        # print()\n",
    "        # print()\n",
    "\n",
    "\n",
    "        self.state=state_nxt\n",
    "\n",
    "        states_record.append(self.state)\n",
    "        self.temp_states_list.append(self.state)\n",
    "\n",
    "\n",
    "        \n",
    "        return self.observation(), self.reward(), done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "global reward_list\n",
    "reward_list=[]\n",
    "\n",
    "global episode_reward_list\n",
    "episode_reward_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 3\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 3\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 3\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 3\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 4\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 4\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 4\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 4\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 5\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 5\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 5\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 5\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 6\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 6\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 6\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 6\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 7\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 7\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 7\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 7\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 8\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 8\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 8\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 8\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 9\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 9\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 9\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 9\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 10\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 10\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 10\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "self.state_nos 10\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 11\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 11\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 11\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 11\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 12\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 12\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 12\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 12\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 13\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 13\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 13\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 13\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 14\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 14\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 14\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 14\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 15\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "self.state_nos 15\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "self.state_nos 15\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 15\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 16\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "self.state_nos 16\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 16\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 16\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 17\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 17\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 17\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 17\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 18\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 18\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 18\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 18\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 19\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 19\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 19\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 19\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 20\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 20\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 20\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 20\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 21\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 21\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 21\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 21\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 22\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 22\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 22\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 22\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 23\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "self.state_nos 23\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 23\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 23\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "self.state_nos 24\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 24\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 24\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 24\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 25\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 25\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 25\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 25\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 26\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 26\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 26\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 26\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 27\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 27\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 27\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 27\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 28\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 28\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "self.state_nos 28\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 28\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 29\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 29\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 29\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 29\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 30\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 30\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 30\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 30\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 31\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 31\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 31\n",
      "Episode Number:  4\n",
      "self.state_nos 31\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "self.state_nos 32\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 32\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 32\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 32\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 33\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 33\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 33\n",
      "Episode Number:  4\n",
      "self.state_nos 33\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 34\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 34\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 34\n",
      "Episode Number:  4\n",
      "self.state_nos 34\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 35\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 35\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 35\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "self.state_nos 35\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 36\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 36\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 36\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 36\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 37\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 37\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "self.state_nos 37\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "self.state_nos 37\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "self.state_nos 38\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "self.state_nos 38\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 38\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 38\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "self.state_nos 39\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 39\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 16ms/step\n",
      "self.state_nos 39\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "self.state_nos 39\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "self.state_nos 40\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 40\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 40\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 40\n",
      "Episode Number:  4\n",
      "arg update started\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 41\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 41\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "self.state_nos 41\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 41\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 17ms/step\n",
      "self.state_nos 42\n",
      "Episode Number:  4\n",
      "1/1 [==============================] - 0s 18ms/step\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "from multiprocessing import cpu_count\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# wandb.init(name='A3C', project=\"deep-rl-tf2\")\n",
    "\n",
    "\n",
    "\n",
    "# args_gamma=0.99\n",
    "# args_update_interval=5\n",
    "# args_actor_lr=0.0005\n",
    "# args_critic_lr=0.001\n",
    "\n",
    "args_gamma=0.99\n",
    "args_update_interval=5\n",
    "args_actor_lr=0.00005\n",
    "args_critic_lr=0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CUR_EPISODE = 0\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args_actor_lr)\n",
    "        self.entropy_beta = 0.01\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(32, activation='relu')(state_input)\n",
    "        dense_2 = Dense(32, activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        # print(\"state\",end=\",\")\n",
    "        # print(state)\n",
    "        # print(\"mu\",end=\",\")\n",
    "        # print(mu)\n",
    "        # print(\"std\",end=\",\")\n",
    "        # print(std)\n",
    "        # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        mu, std = mu[0], std[0]\n",
    "        \n",
    "        return np.random.normal(mu, std, size=self.action_dim)\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, mu, std, actions, advantages):\n",
    "        log_policy_pdf = self.log_pdf(mu, std, actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            loss = self.compute_loss(mu, std, actions, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args_critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name):\n",
    "        env = gym.make(env_name)\n",
    "        if use_custom:\n",
    "            env = gym_env(filename=\"data1.csv\")\n",
    "        self.env_name = env_name       \n",
    "        \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "        self.action_bound = 1.000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.global_actor = Actor(\n",
    "            self.state_dim, self.action_dim, self.action_bound, self.std_bound)\n",
    "        self.global_critic = Critic(self.state_dim)\n",
    "        self.num_workers = 4\n",
    "\n",
    "    def train(self, max_episodes=100):\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            env = gym.make(self.env_name)\n",
    "            if use_custom:\n",
    "                env = gym_env(filename=\"data\"+str(i+1)+\".csv\")\n",
    "                print(\"data\"+str(i+1)+\".csv\")\n",
    "            agentindex=i\n",
    "            workers.append(WorkerAgent(\n",
    "                env, self.global_actor, self.global_critic, max_episodes,agentindex))\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "\n",
    "class WorkerAgent(Thread):\n",
    "    def __init__(self, env, global_actor, global_critic, max_episodes,agentindex):\n",
    "        Thread.__init__(self)\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = 1.000\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.max_episodes = max_episodes\n",
    "        self.agentindex=agentindex\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        self.actor = Actor(self.state_dim, self.action_dim,\n",
    "                           self.action_bound, self.std_bound)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "\n",
    "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = args_gamma * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "    def advatnage(self, td_targets, baselines):\n",
    "        return td_targets - baselines\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self):\n",
    "        global CUR_EPISODE\n",
    "        \n",
    "\n",
    "        while self.max_episodes >= CUR_EPISODE:\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            dict_row_list=[]\n",
    "            episode_reward_list_temp=[]\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # self.env.render()\n",
    "                if use_custom:\n",
    "                    action = self.actor.get_action(state)\n",
    "                else:\n",
    "                    action = self.actor.get_action(state)[0]\n",
    "                \n",
    "                \n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                \n",
    "                #print(self.env.step(action))\n",
    "                \n",
    "                if use_custom:\n",
    "                    next_state, reward, done, info = self.env.step(action)\n",
    "                else:\n",
    "                    next_state, reward, done, info,_ = self.env.step(action)                    \n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append(reward)\n",
    "                # print(\"State :\",end=\"\")\n",
    "                # print(state)\n",
    "                # print(\"action :\",end=\"\")\n",
    "                # print(action)\n",
    "                # print(\"reward :\",end=\"\")\n",
    "                # print(reward)\n",
    "                # print(\"agentindex:\", self.agentindex)\n",
    "                print(\"Episode Number: \", CUR_EPISODE)\n",
    "                \n",
    "                row_dict = {'burst_allowance':state[0][0],'drop_prob':state[0][1],'current_qdelay':state[0][2],'qdelay_old':state[0][3],\n",
    "                            'accu_prob':state[0][4],'measurement_start':state[0][5],\n",
    "                            'tot_pkts':state[0][6],'tot_bytes':state[0][7],\n",
    "                            'length':state[0][8],'len_bytes':state[0][9],\n",
    "                            'drops':state[0][10],'ecn':state[0][11],\n",
    "                            'action': action[0][0],'reward': reward[0][0],\n",
    "                }\n",
    "                dict_row_list.append(row_dict)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                reward_list.append(reward)\n",
    "\n",
    "                if len(state_batch) >= args_update_interval or done:\n",
    "                    print(\"arg update started\")\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "                    td_targets = self.n_step_td_target(\n",
    "                        (rewards+8)/8, next_v_value, done)\n",
    "                    advantages = td_targets - self.critic.model.predict(states)\n",
    "\n",
    "                    actor_loss = self.global_actor.train(\n",
    "                        states, actions, advantages)\n",
    "                    critic_loss = self.global_critic.train(\n",
    "                        states, td_targets)\n",
    "\n",
    "                    self.actor.model.set_weights(\n",
    "                        self.global_actor.model.get_weights())\n",
    "                    self.critic.model.set_weights(\n",
    "                        self.global_critic.model.get_weights())\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    td_target_batch = []\n",
    "                    advatnage_batch = []                \n",
    "                state = next_state[0]\n",
    "                # print(\"asdasdasdsaasdasdd\",end=\"\")\n",
    "                # print(next_state.shape)\n",
    "                # print(next_state[0])\n",
    "\n",
    "            episode_reward += reward[0][0]\n",
    "            print('EP{} EpisodeReward={}'.format(CUR_EPISODE, episode_reward))\n",
    "            # wandb.log({'Reward': episode_reward})\n",
    "            episode_reward_list_temp.append(episode_reward)\n",
    "            CUR_EPISODE += 1\n",
    "        dftemp=pd.DataFrame(dict_row_list)\n",
    "        dftemp.to_csv(\"ActionStateRewardlog\"+str(self.agentindex)+\".csv\")\n",
    "        print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "        dftemp=pd.DataFrame(episode_reward_list_temp, columns =['EpisodeReward'])\n",
    "        dftemp.to_csv(\"RewardWorker\"+str(self.agentindex)+\".csv\")\n",
    "        print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_name = 'Pendulum-v1'\n",
    "    agent = Agent(env_name)\n",
    "    agent.train(100)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rewards=[]\n",
    "for index in range(len(reward_list)):\n",
    "    list_of_rewards.append(reward_list[index][0][0])\n",
    "re_np=np.array(list_of_rewards)\n",
    "import pandas as pd\n",
    "reward_np_list={\n",
    "    \"rewards\":re_np\n",
    "}\n",
    "df=pd.DataFrame(reward_np_list)\n",
    "\n",
    "df.to_csv(\"data_reward.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward_list\n",
    "re_np=np.array(episode_reward_list)\n",
    "import pandas as pd\n",
    "reward_np_list={\n",
    "    \"rewards\":re_np\n",
    "}\n",
    "df=pd.DataFrame(reward_np_list)\n",
    "\n",
    "df.to_csv(\"episode_reward.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
