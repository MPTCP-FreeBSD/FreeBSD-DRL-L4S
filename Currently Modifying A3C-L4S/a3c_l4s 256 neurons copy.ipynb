{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pandas as pd\n",
    "from gym import wrappers\n",
    "from gym import spaces\n",
    "import math\n",
    "import random\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "global no_of_steps\n",
    "no_of_steps=200\n",
    "global total_episodes_to_run\n",
    "total_episodes_to_run=30\n",
    "global last_ep_no_of_steps\n",
    "last_ep_no_of_steps=3500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_custom=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sklearn=pd.read_csv(\"data1.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>burst_allowance</th>\n",
       "      <th>drop_prob</th>\n",
       "      <th>current_qdelay</th>\n",
       "      <th>qdelay_old</th>\n",
       "      <th>accu_prob</th>\n",
       "      <th>measurement_start</th>\n",
       "      <th>tot_pkts</th>\n",
       "      <th>tot_bytes</th>\n",
       "      <th>length</th>\n",
       "      <th>len_bytes</th>\n",
       "      <th>drops</th>\n",
       "      <th>ecn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001496</td>\n",
       "      <td>0.000392</td>\n",
       "      <td>0.000287</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002494</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.011198</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.999988</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.001618</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.019544</td>\n",
       "      <td>0.018578</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.888889</td>\n",
       "      <td>-0.999990</td>\n",
       "      <td>-0.3125</td>\n",
       "      <td>0.020833</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.003257</td>\n",
       "      <td>0.003690</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4750</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.6250</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.195665</td>\n",
       "      <td>0.247684</td>\n",
       "      <td>0.022801</td>\n",
       "      <td>0.025829</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4751</th>\n",
       "      <td>0.555556</td>\n",
       "      <td>-0.999971</td>\n",
       "      <td>-1.2500</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.087948</td>\n",
       "      <td>0.042636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4752</th>\n",
       "      <td>0.444444</td>\n",
       "      <td>-0.999920</td>\n",
       "      <td>-1.2500</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.136900</td>\n",
       "      <td>0.027735</td>\n",
       "      <td>0.035831</td>\n",
       "      <td>0.022779</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>-0.999985</td>\n",
       "      <td>-0.9375</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998670</td>\n",
       "      <td>0.195739</td>\n",
       "      <td>0.247724</td>\n",
       "      <td>0.013029</td>\n",
       "      <td>0.008369</td>\n",
       "      <td>0.435841</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4754</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.998545</td>\n",
       "      <td>0.137096</td>\n",
       "      <td>0.027774</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4755 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      burst_allowance  drop_prob  current_qdelay  qdelay_old  accu_prob  \\\n",
       "0            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "1            1.000000  -1.000000         -0.0000    0.000000        0.0   \n",
       "2            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "3            1.000000  -0.999988         -0.3125    0.020833        0.0   \n",
       "4            0.888889  -0.999990         -0.3125    0.020833        0.0   \n",
       "...               ...        ...             ...         ...        ...   \n",
       "4750         0.777778  -1.000000         -0.6250    0.041667        0.0   \n",
       "4751         0.555556  -0.999971         -1.2500    0.083333        0.0   \n",
       "4752         0.444444  -0.999920         -1.2500    0.083333        0.0   \n",
       "4753         0.666667  -0.999985         -0.9375    0.062500        0.0   \n",
       "4754         0.333333  -1.000000         -0.0000    0.000000        0.0   \n",
       "\n",
       "      measurement_start  tot_pkts  tot_bytes    length  len_bytes     drops  \\\n",
       "0              0.001496  0.000392   0.000287  0.003257   0.003690  0.000000   \n",
       "1              0.001621  0.000637   0.000568  0.000000   0.000000  0.000000   \n",
       "2              0.002494  0.001201   0.001013  0.013029   0.011198  0.000000   \n",
       "3              0.002660  0.001618   0.001512  0.019544   0.018578  0.000000   \n",
       "4              0.002660  0.001716   0.001636  0.003257   0.003690  0.000000   \n",
       "...                 ...       ...        ...       ...        ...       ...   \n",
       "4750           0.998670  0.195665   0.247684  0.022801   0.025829  0.435841   \n",
       "4751           0.998545  0.136900   0.027735  0.087948   0.042636  0.000000   \n",
       "4752           0.998545  0.136900   0.027735  0.035831   0.022779  0.000000   \n",
       "4753           0.998670  0.195739   0.247724  0.013029   0.008369  0.435841   \n",
       "4754           0.998545  0.137096   0.027774  0.000000   0.000000  0.000000   \n",
       "\n",
       "      ecn  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "...   ...  \n",
       "4750    1  \n",
       "4751    0  \n",
       "4752    0  \n",
       "4753    1  \n",
       "4754    0  \n",
       "\n",
       "[4755 rows x 12 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Custom Gym Environment\n",
    "observations = ['burst_allowance',\n",
    " 'drop_prob',\n",
    " 'current_qdelay',\n",
    " 'qdelay_old',\n",
    " 'accu_prob',\n",
    " 'measurement_start',\n",
    " 'tot_pkts',\n",
    " 'tot_bytes',\n",
    " 'length',\n",
    " 'len_bytes',\n",
    " 'drops',\n",
    " 'ecn']\n",
    "\n",
    "observations=list(df_sklearn.columns)\n",
    "\n",
    "actions=['new_drop_prob']\n",
    "\n",
    "states_record=[]\n",
    "\n",
    "states_dict={}\n",
    "\n",
    "# global df_sklearn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def tcp_obs_space():\n",
    "    lower_obs_bound = {\n",
    "        'burst_allowance': - np.inf,\n",
    "        'drop_prob': - np.inf,\n",
    "        'current_qdelay': - np.inf,\n",
    "        'qdelay_old': - np.inf,\n",
    "        'accu_prob': - np.inf,\n",
    "        'measurement_start': - np.inf,\n",
    "        'tot_pkts': - np.inf,\n",
    "        'tot_bytes': - np.inf,\n",
    "        'length': - np.inf,\n",
    "        'len_bytes': - np.inf,\n",
    "        'drops': - np.inf,\n",
    "        'ecn': - np.inf\n",
    "        \n",
    "    }\n",
    "\n",
    "    higher_obs_bound = {\n",
    "        'burst_allowance': np.inf,\n",
    "        'drop_prob': np.inf,\n",
    "        'current_qdelay': np.inf,\n",
    "        'qdelay_old': np.inf,\n",
    "        'accu_prob': np.inf,\n",
    "        'measurement_start': np.inf,\n",
    "        'tot_pkts': np.inf,\n",
    "        'tot_bytes': np.inf,\n",
    "        'length': np.inf,\n",
    "        'len_bytes': np.inf,\n",
    "        'drops': np.inf,\n",
    "        'ecn': np.inf\n",
    "        \n",
    "    }\n",
    "\n",
    "    low = np.array([lower_obs_bound[o] for o in observations])\n",
    "    high = np.array([higher_obs_bound[o] for o in observations])\n",
    "    shape = (len(observations),)\n",
    "    return gym.spaces.Box(low,high,shape)\n",
    "\n",
    "\n",
    "def tcp_action_space():\n",
    "    lower_obs_bound = {\n",
    "        'new_drop_prob': - np.inf,\n",
    "    }\n",
    "    higher_obs_bound = {\n",
    "        'new_drop_prob': np.inf,\n",
    "    }\n",
    "\n",
    "    low = np.array([lower_obs_bound[o] for o in actions])\n",
    "    high = np.array([higher_obs_bound[o] for o in actions])\n",
    "    shape = (len(observations),)\n",
    "    return gym.spaces.Box(low,high,shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gym_env_entire():\n",
    "    \"\"\" \"\"\"\n",
    "    def __init__(self,filename):\n",
    "\n",
    "        self.filename=filename\n",
    "        \n",
    "        self.df_sklearn=pd.read_csv(filename,index_col=0)\n",
    "\n",
    "        self.observations = ['burst_allowance','drop_prob','current_qdelay','qdelay_old','accu_prob',\n",
    "                             'measurement_start','tot_pkts','tot_bytes','length','len_bytes','drops','ecn']\n",
    "        \n",
    "        self.observation_space = tcp_obs_space()\n",
    "\n",
    "        self.count=0\n",
    "\n",
    "        self.state_nos=0\n",
    "        self.temp_states_list=[]\n",
    "        self.ep_count=0\n",
    "        self.action_space = spaces.Box(low=-np.inf,high=np.inf,dtype=np.float32,shape=(1,))\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    def observation(self):\n",
    "        return np.array([self.state[o] for o in self.observations])\n",
    "\n",
    "    def reward(self):\n",
    "        return self.state['current_qdelay']\n",
    "\n",
    "    \"\"\" reset env, return the initial state  \"\"\"\n",
    "    def reset(self):\n",
    "        time.sleep(1)\n",
    "        rindex=random.randint(0, self.df_sklearn.shape[0]-1)\n",
    "        print(\"before increment reset ep_socunt, \",self.ep_count)\n",
    "        self.ep_count+=1\n",
    "        print(\"reset ep_socunt, \",self.ep_count)\n",
    "        if self.ep_count == total_episodes_to_run-1:\n",
    "            print(\"ENtered reset if for last\")\n",
    "            rindex=random.randint(0, 100)\n",
    "        \n",
    "        self.state={\n",
    "            'burst_allowance': self.df_sklearn.iloc[rindex][\"burst_allowance\"],\n",
    "            'drop_prob': df_sklearn.iloc[rindex][\"drop_prob\"],\n",
    "            'current_qdelay': self.df_sklearn.iloc[rindex][\"current_qdelay\"],\n",
    "            'qdelay_old': self.df_sklearn.iloc[rindex][\"qdelay_old\"],\n",
    "            'accu_prob': self.df_sklearn.iloc[rindex][\"accu_prob\"],\n",
    "            'measurement_start': self.df_sklearn.iloc[rindex][\"measurement_start\"],\n",
    "            'tot_pkts': self.df_sklearn.iloc[rindex][\"tot_pkts\"],\n",
    "            'tot_bytes': self.df_sklearn.iloc[rindex][\"tot_bytes\"],\n",
    "            'length': self.df_sklearn.iloc[rindex][\"length\"],\n",
    "            'len_bytes': self.df_sklearn.iloc[rindex][\"len_bytes\"],\n",
    "            'drops': self.df_sklearn.iloc[rindex][\"drops\"],\n",
    "            'ecn': self.df_sklearn.iloc[rindex][\"ecn\"]\n",
    "        }\n",
    "\n",
    "        print(\"reset happened\")\n",
    "        print(self.df_sklearn.iloc[rindex]['current_qdelay'])\n",
    "        self.state_nos=0\n",
    "        self.temp_states_list=[]\n",
    "        self.count=self.count+1\n",
    "\n",
    "        states_record.append(self.state)\n",
    "        self.temp_states_list.append(self.state)\n",
    "        return self.observation()\n",
    "        \n",
    "\n",
    "    \"\"\" action = [sub1_buff_size, sub2_buff_size] \"\"\"\n",
    "    def step(self, action):\n",
    "        # A = [self.fd, action[0], action[1]]\n",
    "        # mpsched.set_seg(A)\n",
    "        time.sleep(2)\n",
    "        done = False       \n",
    "        \n",
    "        #state_nxt = mpsched.get_sub_info(self.fd)\n",
    "\n",
    "        input=action[0]\n",
    "\n",
    "        if(math.isnan(input) or input<-1 or input >1):\n",
    "          input=0\n",
    "          print(\"step happened\")\n",
    "          print(\"Wrong action value\")\n",
    "          print(input)\n",
    "          return self.observation(), self.reward(), done, {}\n",
    "\n",
    "\n",
    "        df_sort = self.df_sklearn.iloc[(self.df_sklearn['drop_prob']-input).abs().argsort()[:1]]\n",
    "\n",
    "\n",
    "        state_nxt={\n",
    "            'burst_allowance': df_sort[\"burst_allowance\"].values[0],\n",
    "            'drop_prob': df_sort[\"drop_prob\"].values[0],\n",
    "            'current_qdelay': df_sort[\"current_qdelay\"].values[0],\n",
    "            'qdelay_old': df_sort[\"qdelay_old\"].values[0],\n",
    "            'accu_prob': df_sort[\"accu_prob\"].values[0],\n",
    "            'measurement_start': df_sort[\"measurement_start\"].values[0],\n",
    "            'tot_pkts': df_sort[\"tot_pkts\"].values[0],\n",
    "            'tot_bytes': df_sort[\"tot_bytes\"].values[0],\n",
    "            'length': df_sort[\"length\"].values[0],\n",
    "            'len_bytes': df_sort[\"len_bytes\"].values[0],\n",
    "            'drops': df_sort[\"drops\"].values[0],\n",
    "            'ecn': df_sort[\"ecn\"].values[0]\n",
    "        }        \n",
    "        \n",
    "        if len(state_nxt) == 0:\n",
    "            done = True\n",
    "\n",
    "\n",
    "        self.state_nos=self.state_nos+1\n",
    "        \n",
    "        if self.ep_count == total_episodes_to_run-1:\n",
    "            print(\"step last episode enetered\")\n",
    "            if self.state_nos > last_ep_no_of_steps:\n",
    "                done=True            \n",
    "        elif(self.ep_count < (total_episodes_to_run -1) and self.state_nos>no_of_steps):\n",
    "            print(\"normal step entered\")\n",
    "            done=True\n",
    "\n",
    "       \n",
    "        if self.state['current_qdelay']==(self.df_sklearn[\"current_qdelay\"].max()+1):\n",
    "            done = True\n",
    "\n",
    "\n",
    "\n",
    "        if done==True:              \n",
    "            states_dict[str(self.count)]=self.temp_states_list\n",
    "            self.temp_states_list=[]\n",
    "\n",
    "        print(\"self.state_nos\",self.state_nos)\n",
    "  \n",
    "\n",
    "        self.state=state_nxt\n",
    "\n",
    "        states_record.append(self.state)\n",
    "        self.temp_states_list.append(self.state)\n",
    "\n",
    "\n",
    "        \n",
    "        return self.observation(), self.reward(), done, {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "global reward_list\n",
    "reward_list=[]\n",
    "\n",
    "global episode_reward_list\n",
    "episode_reward_list=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    return self._getitem_axis(maybe_callable, axis=axis)\n",
      "  File \"c:\\Users\\deols\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1752, in _getitem_axis\n",
      "    self._validate_integer(key, axis)\n",
      "  File \"c:\\Users\\deols\\anaconda3\\envs\\tfenv\\lib\\site-packages\\pandas\\core\\indexing.py\", line 1685, in _validate_integer\n",
      "    raise IndexError(\"single positional indexer is out-of-bounds\")\n",
      "IndexError: single positional indexer is out-of-bounds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 235ms/step\n",
      "1/1 [==============================] - 0s 255ms/step\n",
      "1/1 [==============================] - 0s 320ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  0\n",
      "self.state_nos 1\n",
      "Episode Number:  0\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "self.state_nos 1\n",
      "Episode Number:  0\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  0\n",
      "self.state_nos 2\n",
      "Episode Number:  0\n",
      "1/1 [==============================] - 0s 18ms/step\n",
      "self.state_nos 2\n",
      "Episode Number:  0\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    }
   ],
   "source": [
    "# import wandb\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "import gym\n",
    "import argparse\n",
    "import numpy as np\n",
    "from threading import Thread\n",
    "from multiprocessing import cpu_count\n",
    "tf.keras.backend.set_floatx('float64')\n",
    "# wandb.init(name='A3C', project=\"deep-rl-tf2\")\n",
    "\n",
    "\n",
    "\n",
    "# args_gamma=0.99\n",
    "# args_update_interval=5\n",
    "# args_actor_lr=0.0005\n",
    "# args_critic_lr=0.001\n",
    "\n",
    "args_gamma=0.99\n",
    "args_update_interval=5\n",
    "args_actor_lr=0.00005\n",
    "args_critic_lr=0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "CUR_EPISODE = 0\n",
    "\n",
    "class Actor:\n",
    "    def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.action_bound = action_bound\n",
    "        self.std_bound = std_bound\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args_actor_lr)\n",
    "        self.entropy_beta = 0.01\n",
    "\n",
    "    def create_model(self):\n",
    "        state_input = Input((self.state_dim,))\n",
    "        dense_1 = Dense(256, activation='relu')(state_input)\n",
    "        dense_2 = Dense(256, activation='relu')(dense_1)\n",
    "        out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "        mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "        std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "        return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "    def get_action(self, state):\n",
    "        state = np.reshape(state, [1, self.state_dim])\n",
    "        mu, std = self.model.predict(state)\n",
    "        # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        # print(\"state\",end=\",\")\n",
    "        # print(state)\n",
    "        # print(\"mu\",end=\",\")\n",
    "        # print(mu)\n",
    "        # print(\"std\",end=\",\")\n",
    "        # print(std)\n",
    "        # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "        mu, std = mu[0], std[0]\n",
    "        \n",
    "        return np.random.normal(mu, std, size=self.action_dim)\n",
    "\n",
    "    def log_pdf(self, mu, std, action):\n",
    "        std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "        var = std ** 2\n",
    "        log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "            var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "        return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "    def compute_loss(self, mu, std, actions, advantages):\n",
    "        log_policy_pdf = self.log_pdf(mu, std, actions)\n",
    "        loss_policy = log_policy_pdf * advantages\n",
    "        return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "    def train(self, states, actions, advantages):\n",
    "        with tf.GradientTape() as tape:\n",
    "            mu, std = self.model(states, training=True)\n",
    "            loss = self.compute_loss(mu, std, actions, advantages)\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Critic:\n",
    "    def __init__(self, state_dim):\n",
    "        self.state_dim = state_dim\n",
    "        self.model = self.create_model()\n",
    "        self.opt = tf.keras.optimizers.Adam(args_critic_lr)\n",
    "\n",
    "    def create_model(self):\n",
    "        return tf.keras.Sequential([\n",
    "            Input((self.state_dim,)),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(256, activation='relu'),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def compute_loss(self, v_pred, td_targets):\n",
    "        mse = tf.keras.losses.MeanSquaredError()\n",
    "        return mse(td_targets, v_pred)\n",
    "\n",
    "    def train(self, states, td_targets):\n",
    "        with tf.GradientTape() as tape:\n",
    "            v_pred = self.model(states, training=True)\n",
    "            assert v_pred.shape == td_targets.shape\n",
    "            loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "        grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "        self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "        return loss\n",
    "\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env_name):\n",
    "        # env = gym.make(env_name)\n",
    "        if use_custom:\n",
    "            local_env = gym_env_entire(filename=\"data1.csv\")\n",
    "             \n",
    "        \n",
    "        self.state_dim = local_env.observation_space.shape[0]\n",
    "        self.action_dim = local_env.action_space.shape[0]\n",
    "        self.action_bound = 1.000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.global_actor = Actor(\n",
    "            self.state_dim, self.action_dim, self.action_bound, self.std_bound)\n",
    "        self.global_critic = Critic(self.state_dim)\n",
    "        self.num_workers = 4\n",
    "\n",
    "    def train(self, max_episodes=100):\n",
    "        workers = []\n",
    "\n",
    "        for i in range(self.num_workers):\n",
    "            # env = gym.make(self.env_name)\n",
    "            if use_custom:\n",
    "                self.env_name = gym_env_entire(filename=\"data\"+str(i+1)+\".csv\")\n",
    "                #print(\"data\"+str(i+1)+\".csv\")\n",
    "            agentindex=i\n",
    "            workers.append(WorkerAgent(\n",
    "                self.env_name, self.global_actor, self.global_critic, max_episodes,agentindex))\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.start()\n",
    "\n",
    "        for worker in workers:\n",
    "            worker.join()\n",
    "\n",
    "\n",
    "class WorkerAgent(Thread):\n",
    "    def __init__(self, env, global_actor, global_critic, max_episodes,agentindex):\n",
    "        Thread.__init__(self)\n",
    "        self.env = env\n",
    "        self.state_dim = self.env.observation_space.shape[0]\n",
    "        self.action_dim = self.env.action_space.shape[0]\n",
    "        self.action_bound = 1.000\n",
    "        self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "        self.max_episodes = max_episodes\n",
    "        self.agentindex=agentindex\n",
    "        self.global_actor = global_actor\n",
    "        self.global_critic = global_critic\n",
    "        self.actor = Actor(self.state_dim, self.action_dim,\n",
    "                           self.action_bound, self.std_bound)\n",
    "        self.critic = Critic(self.state_dim)\n",
    "\n",
    "        self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "        self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "    def n_step_td_target(self, rewards, next_v_value, done):\n",
    "        td_targets = np.zeros_like(rewards)\n",
    "        cumulative = 0\n",
    "        if not done:\n",
    "            cumulative = next_v_value\n",
    "\n",
    "        for k in reversed(range(0, len(rewards))):\n",
    "            cumulative = args_gamma * cumulative + rewards[k]\n",
    "            td_targets[k] = cumulative\n",
    "        return td_targets\n",
    "\n",
    "    def advatnage(self, td_targets, baselines):\n",
    "        return td_targets - baselines\n",
    "\n",
    "    def list_to_batch(self, list):\n",
    "        batch = list[0]\n",
    "        for elem in list[1:]:\n",
    "            batch = np.append(batch, elem, axis=0)\n",
    "        return batch\n",
    "\n",
    "    def train(self):\n",
    "        global CUR_EPISODE\n",
    "        \n",
    "\n",
    "        while self.max_episodes >= CUR_EPISODE:\n",
    "            state_batch = []\n",
    "            action_batch = []\n",
    "            reward_batch = []\n",
    "            dict_row_list=[]\n",
    "            episode_reward_list_temp=[]\n",
    "            episode_reward, done = 0, False\n",
    "\n",
    "            state = self.env.reset()\n",
    "            clear_output(wait=True)\n",
    "            \n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # self.env.render()\n",
    "                if use_custom:\n",
    "                    action = self.actor.get_action(state)\n",
    "                else:\n",
    "                    action = self.actor.get_action(state)[0]\n",
    "                \n",
    "                \n",
    "                action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                \n",
    "                #print(self.env.step(action))\n",
    "                \n",
    "                if use_custom:\n",
    "                    next_state, reward, done, info = self.env.step(action)\n",
    "                else:\n",
    "                    next_state, reward, done, info,_ = self.env.step(action)                    \n",
    "\n",
    "                state = np.reshape(state, [1, self.state_dim])\n",
    "                action = np.reshape(action, [1, 1])\n",
    "                next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "                reward = np.reshape(reward, [1, 1])\n",
    "\n",
    "                state_batch.append(state)\n",
    "                action_batch.append(action)\n",
    "                reward_batch.append(reward)\n",
    "                # print(\"State :\",end=\"\")\n",
    "                # print(state)\n",
    "                # print(\"action :\",end=\"\")\n",
    "                # print(action)\n",
    "                # print(\"reward :\",end=\"\")\n",
    "                # print(reward)\n",
    "                # print(\"agentindex:\", self.agentindex)\n",
    "                print(\"Episode Number: \", CUR_EPISODE)\n",
    "                \n",
    "                row_dict = {'burst_allowance':state[0][0],'drop_prob':state[0][1],'current_qdelay':state[0][2],'qdelay_old':state[0][3],\n",
    "                            'accu_prob':state[0][4],'measurement_start':state[0][5],\n",
    "                            'tot_pkts':state[0][6],'tot_bytes':state[0][7],\n",
    "                            'length':state[0][8],'len_bytes':state[0][9],\n",
    "                            'drops':state[0][10],'ecn':state[0][11],\n",
    "                            'action': action[0][0],'reward': reward[0][0],\n",
    "                }\n",
    "                dict_row_list.append(row_dict)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "                reward_list.append(reward)\n",
    "\n",
    "                if len(state_batch) >= args_update_interval or done:\n",
    "                    print(\"arg update started\")\n",
    "                    states = self.list_to_batch(state_batch)\n",
    "                    actions = self.list_to_batch(action_batch)\n",
    "                    rewards = self.list_to_batch(reward_batch)\n",
    "\n",
    "                    next_v_value = self.critic.model.predict(next_state)\n",
    "                    td_targets = self.n_step_td_target(\n",
    "                        (rewards+8)/8, next_v_value, done)\n",
    "                    advantages = td_targets - self.critic.model.predict(states)\n",
    "\n",
    "                    actor_loss = self.global_actor.train(\n",
    "                        states, actions, advantages)\n",
    "                    critic_loss = self.global_critic.train(\n",
    "                        states, td_targets)\n",
    "\n",
    "                    self.actor.model.set_weights(\n",
    "                        self.global_actor.model.get_weights())\n",
    "                    self.critic.model.set_weights(\n",
    "                        self.global_critic.model.get_weights())\n",
    "\n",
    "                    state_batch = []\n",
    "                    action_batch = []\n",
    "                    reward_batch = []\n",
    "                    td_target_batch = []\n",
    "                    advatnage_batch = []                \n",
    "                state = next_state[0]\n",
    "                # print(\"asdasdasdsaasdasdd\",end=\"\")\n",
    "                # print(next_state.shape)\n",
    "                # print(next_state[0])\n",
    "\n",
    "            episode_reward += reward[0][0]\n",
    "            print('EP{} EpisodeReward={}'.format(CUR_EPISODE, episode_reward))\n",
    "            # wandb.log({'Reward': episode_reward})\n",
    "            episode_reward_list_temp.append(episode_reward)\n",
    "            CUR_EPISODE += 1\n",
    "        dftemp=pd.DataFrame(dict_row_list)\n",
    "        dftemp.to_csv(\"ActionStateRewardlog\"+str(self.agentindex)+\".csv\")\n",
    "        print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "        dftemp=pd.DataFrame(episode_reward_list_temp, columns =['EpisodeReward'])\n",
    "        dftemp.to_csv(\"RewardWorker\"+str(self.agentindex)+\".csv\")\n",
    "        print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.train()\n",
    "\n",
    "\n",
    "def main():\n",
    "    env_name = 'L4S'\n",
    "    agent = Agent(env_name)\n",
    "    agent.train(total_episodes_to_run)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_rewards=[]\n",
    "for index in range(len(reward_list)):\n",
    "    list_of_rewards.append(reward_list[index][0][0])\n",
    "re_np=np.array(list_of_rewards)\n",
    "import pandas as pd\n",
    "reward_np_list={\n",
    "    \"rewards\":re_np\n",
    "}\n",
    "df=pd.DataFrame(reward_np_list)\n",
    "\n",
    "df.to_csv(\"data_reward.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_reward_list\n",
    "re_np=np.array(episode_reward_list)\n",
    "import pandas as pd\n",
    "reward_np_list={\n",
    "    \"rewards\":re_np\n",
    "}\n",
    "df=pd.DataFrame(reward_np_list)\n",
    "\n",
    "df.to_csv(\"episode_reward.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 321\u001b[0m\n\u001b[0;32m    317\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain(total_episodes_to_run)\n\u001b[0;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 321\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 316\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    315\u001b[0m     env_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPendulum-v1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 316\u001b[0m     agent \u001b[38;5;241m=\u001b[39m \u001b[43mAgent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    317\u001b[0m     agent\u001b[38;5;241m.\u001b[39mtrain(total_episodes_to_run)\n",
      "Cell \u001b[1;32mIn[24], line 127\u001b[0m, in \u001b[0;36mAgent.__init__\u001b[1;34m(self, env_name)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_name):\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# env = gym.make(env_name)\u001b[39;00m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# if use_custom:\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;66;03m#     env = gym_env(filename=\"data1.csv\")\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv_name \u001b[38;5;241m=\u001b[39m env_name       \n\u001b[1;32m--> 127\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate_dim \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241m.\u001b[39mobservation_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_bound \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.000\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# # import wandb\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.layers import Input, Dense, Lambda\n",
    "\n",
    "# import gym\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# from threading import Thread\n",
    "# from multiprocessing import cpu_count\n",
    "# tf.keras.backend.set_floatx('float64')\n",
    "# # wandb.init(name='A3C', project=\"deep-rl-tf2\")\n",
    "\n",
    "\n",
    "\n",
    "# # args_gamma=0.99\n",
    "# # args_update_interval=5\n",
    "# # args_actor_lr=0.0005\n",
    "# # args_critic_lr=0.001\n",
    "\n",
    "# args_gamma=0.99\n",
    "# args_update_interval=5\n",
    "# args_actor_lr=0.00005\n",
    "# args_critic_lr=0.0001\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# CUR_EPISODE = 0\n",
    "\n",
    "# class Actor:\n",
    "#     def __init__(self, state_dim, action_dim, action_bound, std_bound):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.action_dim = action_dim\n",
    "#         self.action_bound = action_bound\n",
    "#         self.std_bound = std_bound\n",
    "#         self.model = self.create_model()\n",
    "#         self.opt = tf.keras.optimizers.Adam(args_actor_lr)\n",
    "#         self.entropy_beta = 0.01\n",
    "\n",
    "#     def create_model(self):\n",
    "#         state_input = Input((self.state_dim,))\n",
    "#         dense_1 = Dense(256, activation='relu')(state_input)\n",
    "#         dense_2 = Dense(256, activation='relu')(dense_1)\n",
    "#         out_mu = Dense(self.action_dim, activation='tanh')(dense_2)\n",
    "#         mu_output = Lambda(lambda x: x * self.action_bound)(out_mu)\n",
    "#         std_output = Dense(self.action_dim, activation='softplus')(dense_2)\n",
    "#         return tf.keras.models.Model(state_input, [mu_output, std_output])\n",
    "\n",
    "#     def get_action(self, state):\n",
    "#         state = np.reshape(state, [1, self.state_dim])\n",
    "#         mu, std = self.model.predict(state)\n",
    "#         # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "#         # print(\"state\",end=\",\")\n",
    "#         # print(state)\n",
    "#         # print(\"mu\",end=\",\")\n",
    "#         # print(mu)\n",
    "#         # print(\"std\",end=\",\")\n",
    "#         # print(std)\n",
    "#         # print(\">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "#         mu, std = mu[0], std[0]\n",
    "        \n",
    "#         return np.random.normal(mu, std, size=self.action_dim)\n",
    "\n",
    "#     def log_pdf(self, mu, std, action):\n",
    "#         std = tf.clip_by_value(std, self.std_bound[0], self.std_bound[1])\n",
    "#         var = std ** 2\n",
    "#         log_policy_pdf = -0.5 * (action - mu) ** 2 / \\\n",
    "#             var - 0.5 * tf.math.log(var * 2 * np.pi)\n",
    "#         return tf.reduce_sum(log_policy_pdf, 1, keepdims=True)\n",
    "\n",
    "#     def compute_loss(self, mu, std, actions, advantages):\n",
    "#         log_policy_pdf = self.log_pdf(mu, std, actions)\n",
    "#         loss_policy = log_policy_pdf * advantages\n",
    "#         return tf.reduce_sum(-loss_policy)\n",
    "\n",
    "#     def train(self, states, actions, advantages):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             mu, std = self.model(states, training=True)\n",
    "#             loss = self.compute_loss(mu, std, actions, advantages)\n",
    "#         grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "#         self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "#         return loss\n",
    "\n",
    "\n",
    "# class Critic:\n",
    "#     def __init__(self, state_dim):\n",
    "#         self.state_dim = state_dim\n",
    "#         self.model = self.create_model()\n",
    "#         self.opt = tf.keras.optimizers.Adam(args_critic_lr)\n",
    "\n",
    "#     def create_model(self):\n",
    "#         return tf.keras.Sequential([\n",
    "#             Input((self.state_dim,)),\n",
    "#             Dense(256, activation='relu'),\n",
    "#             Dense(256, activation='relu'),\n",
    "#             Dense(64, activation='relu'),\n",
    "#             Dense(16, activation='relu'),\n",
    "#             Dense(1, activation='linear')\n",
    "#         ])\n",
    "\n",
    "#     def compute_loss(self, v_pred, td_targets):\n",
    "#         mse = tf.keras.losses.MeanSquaredError()\n",
    "#         return mse(td_targets, v_pred)\n",
    "\n",
    "#     def train(self, states, td_targets):\n",
    "#         with tf.GradientTape() as tape:\n",
    "#             v_pred = self.model(states, training=True)\n",
    "#             assert v_pred.shape == td_targets.shape\n",
    "#             loss = self.compute_loss(v_pred, tf.stop_gradient(td_targets))\n",
    "#         grads = tape.gradient(loss, self.model.trainable_variables)\n",
    "#         self.opt.apply_gradients(zip(grads, self.model.trainable_variables))\n",
    "#         return loss\n",
    "\n",
    "\n",
    "# class Agent:\n",
    "#     def __init__(self, env_name):\n",
    "#         # env = gym.make(env_name)\n",
    "#         # if use_custom:\n",
    "#         #     env = gym_env(filename=\"data1.csv\")\n",
    "#         self.env_name = env_name       \n",
    "        \n",
    "#         self.state_dim = env.observation_space.shape[0]\n",
    "#         self.action_dim = env.action_space.shape[0]\n",
    "#         self.action_bound = 1.000\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "#         self.global_actor = Actor(\n",
    "#             self.state_dim, self.action_dim, self.action_bound, self.std_bound)\n",
    "#         self.global_critic = Critic(self.state_dim)\n",
    "#         self.num_workers = 4\n",
    "\n",
    "#     def train(self, max_episodes=100):\n",
    "#         workers = []\n",
    "\n",
    "#         for i in range(self.num_workers):\n",
    "#             # env = gym.make(self.env_name)\n",
    "#             if use_custom:\n",
    "#                 self.env = gym_env_entire(filename=\"data\"+str(i+1)+\".csv\")\n",
    "#                 #print(\"data\"+str(i+1)+\".csv\")\n",
    "#             agentindex=i\n",
    "#             workers.append(WorkerAgent(\n",
    "#                 self.env, self.global_actor, self.global_critic, max_episodes,agentindex))\n",
    "\n",
    "#         for worker in workers:\n",
    "#             worker.start()\n",
    "\n",
    "#         for worker in workers:\n",
    "#             worker.join()\n",
    "\n",
    "\n",
    "# class WorkerAgent(Thread):\n",
    "#     def __init__(self, env, global_actor, global_critic, max_episodes,agentindex):\n",
    "#         Thread.__init__(self)\n",
    "#         self.env = env\n",
    "#         self.state_dim = self.env.observation_space.shape[0]\n",
    "#         self.action_dim = self.env.action_space.shape[0]\n",
    "#         self.action_bound = 1.000\n",
    "#         self.std_bound = [1e-2, 1.0]\n",
    "\n",
    "#         self.max_episodes = max_episodes\n",
    "#         self.agentindex=agentindex\n",
    "#         self.global_actor = global_actor\n",
    "#         self.global_critic = global_critic\n",
    "#         self.actor = Actor(self.state_dim, self.action_dim,\n",
    "#                            self.action_bound, self.std_bound)\n",
    "#         self.critic = Critic(self.state_dim)\n",
    "\n",
    "#         self.actor.model.set_weights(self.global_actor.model.get_weights())\n",
    "#         self.critic.model.set_weights(self.global_critic.model.get_weights())\n",
    "\n",
    "#     def n_step_td_target(self, rewards, next_v_value, done):\n",
    "#         td_targets = np.zeros_like(rewards)\n",
    "#         cumulative = 0\n",
    "#         if not done:\n",
    "#             cumulative = next_v_value\n",
    "\n",
    "#         for k in reversed(range(0, len(rewards))):\n",
    "#             cumulative = args_gamma * cumulative + rewards[k]\n",
    "#             td_targets[k] = cumulative\n",
    "#         return td_targets\n",
    "\n",
    "#     def advatnage(self, td_targets, baselines):\n",
    "#         return td_targets - baselines\n",
    "\n",
    "#     def list_to_batch(self, list):\n",
    "#         batch = list[0]\n",
    "#         for elem in list[1:]:\n",
    "#             batch = np.append(batch, elem, axis=0)\n",
    "#         return batch\n",
    "\n",
    "#     def train(self):\n",
    "#         global CUR_EPISODE\n",
    "        \n",
    "\n",
    "#         while self.max_episodes >= CUR_EPISODE:\n",
    "#             state_batch = []\n",
    "#             action_batch = []\n",
    "#             reward_batch = []\n",
    "#             dict_row_list=[]\n",
    "#             episode_reward_list_temp=[]\n",
    "#             episode_reward, done = 0, False\n",
    "\n",
    "#             state = self.env.reset()\n",
    "#             clear_output(wait=True)\n",
    "            \n",
    "\n",
    "#             while not done:\n",
    "                \n",
    "#                 # self.env.render()\n",
    "#                 if use_custom:\n",
    "#                     action = self.actor.get_action(state)\n",
    "#                 else:\n",
    "#                     action = self.actor.get_action(state)[0]\n",
    "                \n",
    "                \n",
    "#                 action = np.clip(action, -self.action_bound, self.action_bound)\n",
    "                \n",
    "#                 #print(self.env.step(action))\n",
    "                \n",
    "#                 if use_custom:\n",
    "#                     next_state, reward, done, info = self.env.step(action)\n",
    "#                 else:\n",
    "#                     next_state, reward, done, info,_ = self.env.step(action)                    \n",
    "\n",
    "#                 state = np.reshape(state, [1, self.state_dim])\n",
    "#                 action = np.reshape(action, [1, 1])\n",
    "#                 next_state = np.reshape(next_state, [1, self.state_dim])\n",
    "#                 reward = np.reshape(reward, [1, 1])\n",
    "\n",
    "#                 state_batch.append(state)\n",
    "#                 action_batch.append(action)\n",
    "#                 reward_batch.append(reward)\n",
    "#                 # print(\"State :\",end=\"\")\n",
    "#                 # print(state)\n",
    "#                 # print(\"action :\",end=\"\")\n",
    "#                 # print(action)\n",
    "#                 # print(\"reward :\",end=\"\")\n",
    "#                 # print(reward)\n",
    "#                 # print(\"agentindex:\", self.agentindex)\n",
    "#                 print(\"Episode Number: \", CUR_EPISODE)\n",
    "                \n",
    "#                 row_dict = {'burst_allowance':state[0][0],'drop_prob':state[0][1],'current_qdelay':state[0][2],'qdelay_old':state[0][3],\n",
    "#                             'accu_prob':state[0][4],'measurement_start':state[0][5],\n",
    "#                             'tot_pkts':state[0][6],'tot_bytes':state[0][7],\n",
    "#                             'length':state[0][8],'len_bytes':state[0][9],\n",
    "#                             'drops':state[0][10],'ecn':state[0][11],\n",
    "#                             'action': action[0][0],'reward': reward[0][0],\n",
    "#                 }\n",
    "#                 dict_row_list.append(row_dict)\n",
    "\n",
    "                \n",
    "                \n",
    "                \n",
    "#                 reward_list.append(reward)\n",
    "\n",
    "#                 if len(state_batch) >= args_update_interval or done:\n",
    "#                     print(\"arg update started\")\n",
    "#                     states = self.list_to_batch(state_batch)\n",
    "#                     actions = self.list_to_batch(action_batch)\n",
    "#                     rewards = self.list_to_batch(reward_batch)\n",
    "\n",
    "#                     next_v_value = self.critic.model.predict(next_state)\n",
    "#                     td_targets = self.n_step_td_target(\n",
    "#                         (rewards+8)/8, next_v_value, done)\n",
    "#                     advantages = td_targets - self.critic.model.predict(states)\n",
    "\n",
    "#                     actor_loss = self.global_actor.train(\n",
    "#                         states, actions, advantages)\n",
    "#                     critic_loss = self.global_critic.train(\n",
    "#                         states, td_targets)\n",
    "\n",
    "#                     self.actor.model.set_weights(\n",
    "#                         self.global_actor.model.get_weights())\n",
    "#                     self.critic.model.set_weights(\n",
    "#                         self.global_critic.model.get_weights())\n",
    "\n",
    "#                     state_batch = []\n",
    "#                     action_batch = []\n",
    "#                     reward_batch = []\n",
    "#                     td_target_batch = []\n",
    "#                     advatnage_batch = []                \n",
    "#                 state = next_state[0]\n",
    "#                 # print(\"asdasdasdsaasdasdd\",end=\"\")\n",
    "#                 # print(next_state.shape)\n",
    "#                 # print(next_state[0])\n",
    "\n",
    "#             episode_reward += reward[0][0]\n",
    "#             print('EP{} EpisodeReward={}'.format(CUR_EPISODE, episode_reward))\n",
    "#             # wandb.log({'Reward': episode_reward})\n",
    "#             episode_reward_list_temp.append(episode_reward)\n",
    "#             CUR_EPISODE += 1\n",
    "#         dftemp=pd.DataFrame(dict_row_list)\n",
    "#         dftemp.to_csv(\"ActionStateRewardlog\"+str(self.agentindex)+\".csv\")\n",
    "#         print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "#         dftemp=pd.DataFrame(episode_reward_list_temp, columns =['EpisodeReward'])\n",
    "#         dftemp.to_csv(\"RewardWorker\"+str(self.agentindex)+\".csv\")\n",
    "#         print(\"*_*_*__*_*_*_*_*_*__*_*_*_*__*_*_*_*__*_*_*__*_*_*_*_*_*_*_*__*_*_*_*_*_*_*_*_*_*_\")\n",
    "\n",
    "\n",
    "#     def run(self):\n",
    "#         self.train()\n",
    "\n",
    "\n",
    "# def main():\n",
    "#     env_name = 'Pendulum-v1'\n",
    "#     agent = Agent(env_name)\n",
    "#     agent.train(total_episodes_to_run)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
